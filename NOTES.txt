* General
## we can set node name to the pod, to bypass scheduler and place the pod on that node directly

* Commands
kubectl run mosquito --image nginx
kubectl run bee --image nginx --dry-run=client -o yaml > bee.yml
kubectl describe node controlplane | grep Taints
kubectl taint node node01 spray=mortein:NoSchedule
kubectl get pods -o wide
kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule- # dash in the end to remove the taint
kubectl label node node01 color=blue
kubectl create deployment blue --image nginx --replicas 3 --dry-run=client -o yaml > blue.yml
kubectl get daemonsets
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"
kubectl create configmap --name my-scheduler-config --form-file=/root/my-scheduler-config.yaml -n kube-system
ps -ef | grep kube-apiserver | grep admission-plugins
kubectl create secret tls webhook-server-tls --cert=/root/keys/webhook-server-tls.crt --key=/root/keys/webhook-server-tls.key
kubectl top node # cpu, memory info on nodes

* Labels and selectors
kubectl get pods --selector env=dev
kubectl get pods --selector env=test --no-headers | wc -l
kubectl get pods --selector bu=finance --no-headers | wc -l
kubectl get all --selector env=prod --no-headers | wc -l

* Taints and tolerations
# only used for restricting nodes to accept/not accept pods
## not used to tell pods to go to particular pod, instead we tell nodes to accept pods with certain tolerations
taints are set to nodes, tolerations are set to pods
kubectl describe node kubemaster | grep Taint
kubectl taint nodes node-name key=value:taint-effect where taint effect is one of: NoSchedule | PreferNoSchedule | NoExecute
kubectl taint node node1 app=blue:NoSchedule
# yaml for pods
tolerations:
- key: app
  operator: "Equal"
  value: "blue"
  effect: "NoSchedule"

* Node selectors
# labeling the node
## telling pods to target specific node
### we cannot utilize advanced expressions like OR or NOT using node selectors
kubectl label node node-name label-key=label-value
kubectl label node node1 size=Large
# yaml
containers:
nodeSelector:
  size: Large

* Node affinity
# ensure that the pods are hosted on particular nodes
## we can utilize advanced expressions like OR or NOT using node affinity
### node affinity types: 
 - requiredDuringSchedulingIgnoredDuringExecution
 - preferredDuringSchedulingIgnoredDuringExecution
 - (planned) requiredDuringSchedulingRequiredDuringExecution
# yaml
containers:
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: size
          operator: In # NotIn | Exists (values not needed for exists)
          values:
          - Large

* Node affinity with the combination of Taints and tolerations
## can be used to achive specific node selection + to avoid other pods being placed to specific nodes

* Resource requirements and limits
1G - Gigabyte = 1 000 000 000 bytes
1M - Megabyte = 1 000 000 bytes
1K - Kilobyte = 1 000 bytes

1Gi - Gibibyte = 1 073 741 824 bytes
1Mi - Mebibyte = 1 048 576 bytes
1Ki - Kibibyte = 1 024 bytes
# yaml
containers:
- name:
  image:
  resources:
    requests:
      memory: 1Gi
      cpu: 1
    limits:
      memory: 2Gi
      cpu: 2
      
* LimitRange
# applicable on namespace level
# yaml
apiVersion: v1
kind: LimitRange
metadata: 
    name: cpu-resource-constraint
spec:
    limits:
    - default:
        cpu: 500m
      defaultRequest:
        cpu: 500m
      max:
        cpu: "1"
      min:
        cpu: 100m
      type: Container

* ResourceQuota
# set hard limits for requests and limits
## used on namespace level
# yaml
apiVersion: v1
kind: ResourceQuota
metadata:
    name: my-resource-quota
spec:
    hard:
      requests.cpu: 4
      requests.memory: 4Gi
      limits.cpu: 10
      limits.memory: 10Gi

* Daemonsets
# Like ReplicaSets, but runs each of the pods on a separate node
## ensures 1 copy of the pod is always present in all nodes in the cluster
kubectl get ds -a

* Static pods
# kubelet managing pods, works at pod level and can manage pods only
## manifests of the pods stored in the directory under
/etc/kubernetes/manifests
### if the manifest file gets deleted from dir, pod is also removed
#### the directory can be any directory. it is set as an argument (--pod-manifest-path) while configuring the service
#### or it can be set as within the config file, where argument is --config, the file is in .yaml format, containing staticPodPath property
#### use case - to deploy apps/pods that do not fall under k8s management
#### to identify a static pod, there will be a -<node_name> in the name of the pod, like: redis-node01
ps -aux | grep kubelet -> to identify how the service has started, check config or --pod-manifest-path to identify manifest location

* Priority classes
# make sure that high Priority workloads always get scheduled without being interrupted by low Priority workloads
# define priorities for workloads
# non-namespace objects
## priorities defined with numbers, range from -2 147 483 648 to 1 000 000 000. system apps range can go up to 2 000 000 000
kubectl get priorityclass
# yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata: 
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
preemptionPolicy: PreemptLowerPriority # or never
### by default pods with lover priority will be evicted from the node if no space left for new priority pods
#### to change this behavior, we can set PreemptLowerPriority to "never"
### we can also define global priority class with "globalDefault" property set to true
## associate priority class definitiona to a pod, default value is 0
## yaml
apiVersion: v1
kind: pod
metadata:
spec:
  containers:
    - name: nginx
      image: nginx
  priorityClassName: high-priority
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

* Multiple schedulers
# we can set up multiple schedulers, they can be deployed as a pod as well
## we can instruct pods to use specific scheduler
# yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler-2
## yaml for pods
containers:
  - image: nginx
    name: nginx
schedulerName: my-custom-scheduler
### to check if pod is scheduled:
kubectl get events -o wide
kubectl logs my-custom-scheduler --name-space=kube-system

* Configuring Scheduler profiles
phases of scheduling:
- Scheduling queue
- Filtering # filter nodes based on requests. nodes with insufficient resources will not be considered
- Scoring # scoring nodes, based on requests. node which has the most available resources, gets picked up
- Binding # binds pod to a node with highest score
scheduling plugins:
- Scheduling queue > PrioritySort
- Filtering > NodeResourcesFit, NodeName, NodeUnschedulable
- Scoring > NodeResourcesFit, ImageLocality
- Binding > DefaultBinder
Kubernetes allows customization of plugins.
That is achieved by Extensions Points.
Extensions:
- Scheduling queue > queueSort
- Filtering > filter
- Scoring > score
- Binding > bind
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

* Admission Controllers
kubectl > 
  authentication (kube config) > 
  authorization (rbac) > 
  admission controllers (enforcing rules) > 
  create pod
# view enabled admission controllers
kube-apiserver -h | grep enable-admission-plugins
## to enable specific plugin for admission controllers
add --enable-admission-plugins argument to the kube-apiserver service start
## to disable specific plugin
add --disable-admission-plugins
### Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated 
  and now replaced by NamespaceLifecycle admission controller.
The NamespaceLifecycle admission controller will make sure that requests
  to a non-existent namespace is rejected and that the default namespaces such as
    default, kube-system and kube-public cannot be deleted.
# check which plugins are enabled or disabled
ps -ef | grep kube-apiserver | grep admission-plugins

* Validating and Mutating Admission Controllers
# validating addmission controllers
## validating addmission controllers are validating the request (allowed/denied)
# mutating addmission controller can change the object itself before creating it
## example: storageClass: default
### custom admission controllers:
 - MutatingAdmissionWebhook
 - ValidatingAdmissionWebhook
#### requires custom admission webhook server deployed
- requires webhook-service
- requires webhook-deployment 
-- if deployed in k8s
# yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: "pod-policy.example.com"
webhooks:
- name: "pod-policy.example.com"
  rules:
  - apiGroups:   [""]
    apiVersions: ["v1"]
    operations:  ["CREATE"]
    resources:   ["pods"]
    scope:       "Namespaced"
  clientConfig:
    # url: "https://my-server-controller" - if deployed outside k8s
    service:
      namespace: "example-namespace"
      name: "example-service"
    caBundle: <CA_BUNDLE> # cert should be added
  admissionReviewVersions: ["v1"]


* Logging and monitoring
# Metrics server, in memory monitoring solution
## kubelet, runs with cAdvisor (sharing the metrics to metrics server)
### minikube addons enable metrics-server
kubectl top node
kubectl top pod
# deploy metrics server:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
# logs
kubectl logs pod/<name> -f
kubectl logs pod/<name> -c <container-name> -f # -f is --follow

* Rolling udates and Rollbacks
# Rollout
## rollout > deployment > revision
kubectl rollout status deployment/my-deployment
kubectl rollout history deployment/my-deployment
# Deployment strategy
* Recreate
* Rolling Update - Default deployement strategy
kubectl apply -f deployment-definition.yaml
kubectl set image deployment/my-app-deployment nginx-container=nginx:1.9.1 # careful
kubectl describe deployment/my-deployment
# Rollback
kubectl rollout undo deployment/my-deployment

* Commands
# Dockerfile
FROM ubuntu
ENTRYPOINT ['sleep']
CMD ['5']
Docker            k8s
Entrypoint  ---> command
CMD         ---> args
# yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubumtu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ['sleepv2']
      args: ['10]

Create ConfigMaps
# imperative
kubectl create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap <config-name> --from-file=app_config.properties
# declerative
kubectl create -f ..
# yaml - set vars directly in pod
apiVersion: v1
kind: ConfigMap
metadata: 
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

## yaml - Inject configmap in pod
  ```
  apiVersion: v1
  kind: Pod
  metadata:
    name: simple-webapp-color
  spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config
### yaml - Inject only specific var from configmap in a pod
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
spec:
  containers:
    - image: kodekloud/webapp-color
      imagePullPolicy: Always
      name: webapp-color
      env:
        - name: APP_COLOR
          valueFrom:
            configMapKeyRef:
                # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
              name: webapp-config-map
                # Specify the key associated with the value
              key: APP_COLOR
#### yaml - Inject only specific var from secret in a pod
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
    env:
      - name: MY_SECRET_VAR
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: mysecretkey

* Secrets
kubectl create secret generic <secret-name> --from-literal=<key>=<value> --from-literal=...
kubectl create secret generic app-secret --from-literal=DB_HOST=mysql
kubectl create secret generic app-secret --form-file=app_secret.properties
echo -n "mysql" | base64
echo -n "cm9vdA==" | base64 --decode

# yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
    envFrom:
      - secretRef:
          name: mysecret

* Multi container pods
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: app
  name: app
  namespace: elastic-stack
spec:
  containers:
  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume
  - name: app
    image: kodekloud/event-simulator
    imagePullPolicy: Always
  dnsPolicy: ClusterFirst
  volumes:
  - hostPath:
      path: /var/log/webapp
      type: DirectoryOrCreate
    name: log-volume

There are 3 common patterns, when it comes to designing multi-container PODs. 
The first and what we just saw with the logging service example is known as a side car pattern. 
The others are the adapter and the ambassador pattern.
But these fall under the CKAD curriculum and are not required for the CKA exam.
So we will be discuss these in more detail in the CKAD course.

* Init containers
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']


* Self Healing Applications
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. 
The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. 
It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. 
However these are not required for the CKA exam and as such they are not covered here. 
These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.

* Autoscaling
HPA, VPA
- requires metrics server running in the background

Horizontal Pod Autoscaler
kubectl autoscale deployment my-app --cpu-percent=50 --min=1 --max=10
(scales the deployment if the cpu rises above 50% setting replicas from 1 to 10)
- kubectl get hpa
# yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 50
status:
  currentReplicas: 0
  desiredReplicas: 0

In place resize of pod resources
as of v1.32 direct pod update results in pod replacement (delete and recreate)
new feature - 1.27 (alpha), not enabled by default
FEATURE_GATES=InPlacePodVerticalScaling=true (to enable)
only cpu and memory resources can be changed

Vertical Pod Autoscaling
kubectl edit deployment/my-app
change resources
VPA does not come built in with Kubernetes, there is no imperative command
we need to deploy it from kubernetes github
Consists of 3 components:
- admission controller
- recommender
- updater
Operates in 4 modes:
- off, only recommends, does not change anything
- initial, only changes on pod creation, not later
- recreate, evicts pods if usage goes beyond range
- auto, updates existing pods to recommended numbers

kubectl describe vpa my-app

# yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: hamster-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: hamster
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
          memory: 50Mi
        maxAllowed:
          cpu: 1
          memory: 500Mi
        controlledResources: ["cpu", "memory"]