* General
## we can set node name to the pod, to bypass scheduler and place the pod on that node directly

* Commands
kubectl run mosquito --image nginx
kubectl run bee --image nginx --dry-run=client -o yaml > bee.yml
kubectl describe node controlplane | grep Taints
kubectl taint node node01 spray=mortein:NoSchedule
kubectl get pods -o wide
kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule- # dash in the end to remove the taint
kubectl label node node01 color=blue
kubectl create deployment blue --image nginx --replicas 3 --dry-run=client -o yaml > blue.yml
kubectl get daemonsets
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"
kubectl create configmap --name my-scheduler-config --form-file=/root/my-scheduler-config.yaml -n kube-system
ps -ef | grep kube-apiserver | grep admission-plugins
kubectl create secret tls webhook-server-tls --cert=/root/keys/webhook-server-tls.crt --key=/root/keys/webhook-server-tls.key
kubectl top node # cpu, memory info on nodes

* Labels and selectors
kubectl get pods --selector env=dev
kubectl get pods --selector env=test --no-headers | wc -l
kubectl get pods --selector bu=finance --no-headers | wc -l
kubectl get all --selector env=prod --no-headers | wc -l

* Taints and tolerations
# only used for restricting nodes to accept/not accept pods
## not used to tell pods to go to particular pod, instead we tell nodes to accept pods with certain tolerations
taints are set to nodes, tolerations are set to pods
kubectl describe node kubemaster | grep Taint
kubectl taint nodes node-name key=value:taint-effect where taint effect is one of: NoSchedule | PreferNoSchedule | NoExecute
kubectl taint node node1 app=blue:NoSchedule
# yaml for pods
tolerations:
- key: app
  operator: "Equal"
  value: "blue"
  effect: "NoSchedule"

* Node selectors
# labeling the node
## telling pods to target specific node
### we cannot utilize advanced expressions like OR or NOT using node selectors
kubectl label node node-name label-key=label-value
kubectl label node node1 size=Large
# yaml
containers:
nodeSelector:
  size: Large

* Node affinity
# ensure that the pods are hosted on particular nodes
## we can utilize advanced expressions like OR or NOT using node affinity
### node affinity types: 
 - requiredDuringSchedulingIgnoredDuringExecution
 - preferredDuringSchedulingIgnoredDuringExecution
 - (planned) requiredDuringSchedulingRequiredDuringExecution
# yaml
containers:
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: size
          operator: In # NotIn | Exists (values not needed for exists)
          values:
          - Large

* Node affinity with the combination of Taints and tolerations
## can be used to achive specific node selection + to avoid other pods being placed to specific nodes

* Resource requirements and limits
1G - Gigabyte = 1 000 000 000 bytes
1M - Megabyte = 1 000 000 bytes
1K - Kilobyte = 1 000 bytes

1Gi - Gibibyte = 1 073 741 824 bytes
1Mi - Mebibyte = 1 048 576 bytes
1Ki - Kibibyte = 1 024 bytes
# yaml
containers:
- name:
  image:
  resources:
    requests:
      memory: 1Gi
      cpu: 1
    limits:
      memory: 2Gi
      cpu: 2
      
* LimitRange
# applicable on namespace level
# yaml
apiVersion: v1
kind: LimitRange
metadata: 
    name: cpu-resource-constraint
spec:
    limits:
    - default:
        cpu: 500m
      defaultRequest:
        cpu: 500m
      max:
        cpu: "1"
      min:
        cpu: 100m
      type: Container

* ResourceQuota
# set hard limits for requests and limits
## used on namespace level
# yaml
apiVersion: v1
kind: ResourceQuota
metadata:
    name: my-resource-quota
spec:
    hard:
      requests.cpu: 4
      requests.memory: 4Gi
      limits.cpu: 10
      limits.memory: 10Gi

* Daemonsets
# Like ReplicaSets, but runs each of the pods on a separate node
## ensures 1 copy of the pod is always present in all nodes in the cluster
kubectl get ds -a

* Static pods
# kubelet managing pods, works at pod level and can manage pods only
## manifests of the pods stored in the directory under
/etc/kubernetes/manifests
### if the manifest file gets deleted from dir, pod is also removed
#### the directory can be any directory. it is set as an argument (--pod-manifest-path) while configuring the service
#### or it can be set as within the config file, where argument is --config, the file is in .yaml format, containing staticPodPath property
#### use case - to deploy apps/pods that do not fall under k8s management
#### to identify a static pod, there will be a -<node_name> in the name of the pod, like: redis-node01
ps -aux | grep kubelet -> to identify how the service has started, check config or --pod-manifest-path to identify manifest location

* Priority classes
# make sure that high Priority workloads always get scheduled without being interrupted by low Priority workloads
# define priorities for workloads
# non-namespace objects
## priorities defined with numbers, range from -2 147 483 648 to 1 000 000 000. system apps range can go up to 2 000 000 000
kubectl get priorityclass
# yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata: 
  name: high-priority
value: 1000000000
description: "Priority class for mission critical pods"
preemptionPolicy: PreemptLowerPriority # or never
### by default pods with lover priority will be evicted from the node if no space left for new priority pods
#### to change this behavior, we can set PreemptLowerPriority to "never"
### we can also define global priority class with "globalDefault" property set to true
## associate priority class definitiona to a pod, default value is 0
## yaml
apiVersion: v1
kind: pod
metadata:
spec:
  containers:
    - name: nginx
      image: nginx
  priorityClassName: high-priority
kubectl get pods -o custom-columns="NAME:.metadata.name,PRIORITY:.spec.priorityClassName"

* Multiple schedulers
# we can set up multiple schedulers, they can be deployed as a pod as well
## we can instruct pods to use specific scheduler
# yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: my-scheduler-2
## yaml for pods
containers:
  - image: nginx
    name: nginx
schedulerName: my-custom-scheduler
### to check if pod is scheduled:
kubectl get events -o wide
kubectl logs my-custom-scheduler --name-space=kube-system

* Configuring Scheduler profiles
phases of scheduling:
- Scheduling queue
- Filtering # filter nodes based on requests. nodes with insufficient resources will not be considered
- Scoring # scoring nodes, based on requests. node which has the most available resources, gets picked up
- Binding # binds pod to a node with highest score
scheduling plugins:
- Scheduling queue > PrioritySort
- Filtering > NodeResourcesFit, NodeName, NodeUnschedulable
- Scoring > NodeResourcesFit, ImageLocality
- Binding > DefaultBinder
Kubernetes allows customization of plugins.
That is achieved by Extensions Points.
Extensions:
- Scheduling queue > queueSort
- Filtering > filter
- Scoring > score
- Binding > bind
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md
https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/
https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/
https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work

* Admission Controllers
kubectl > 
  authentication (kube config) > 
  authorization (rbac) > 
  admission controllers (enforcing rules) > 
  create pod
# view enabled admission controllers
kube-apiserver -h | grep enable-admission-plugins
## to enable specific plugin for admission controllers
add --enable-admission-plugins argument to the kube-apiserver service start
## to disable specific plugin
add --disable-admission-plugins
### Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated 
  and now replaced by NamespaceLifecycle admission controller.
The NamespaceLifecycle admission controller will make sure that requests
  to a non-existent namespace is rejected and that the default namespaces such as
    default, kube-system and kube-public cannot be deleted.
# check which plugins are enabled or disabled
ps -ef | grep kube-apiserver | grep admission-plugins

* Validating and Mutating Admission Controllers
# validating addmission controllers
## validating addmission controllers are validating the request (allowed/denied)
# mutating addmission controller can change the object itself before creating it
## example: storageClass: default
### custom admission controllers:
 - MutatingAdmissionWebhook
 - ValidatingAdmissionWebhook
#### requires custom admission webhook server deployed
- requires webhook-service
- requires webhook-deployment 
-- if deployed in k8s
# yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: "pod-policy.example.com"
webhooks:
- name: "pod-policy.example.com"
  rules:
  - apiGroups:   [""]
    apiVersions: ["v1"]
    operations:  ["CREATE"]
    resources:   ["pods"]
    scope:       "Namespaced"
  clientConfig:
    # url: "https://my-server-controller" - if deployed outside k8s
    service:
      namespace: "example-namespace"
      name: "example-service"
    caBundle: <CA_BUNDLE> # cert should be added
  admissionReviewVersions: ["v1"]


* Logging and monitoring
# Metrics server, in memory monitoring solution
## kubelet, runs with cAdvisor (sharing the metrics to metrics server)
### minikube addons enable metrics-server
kubectl top node
kubectl top pod
# deploy metrics server:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
# logs
kubectl logs pod/<name> -f
kubectl logs pod/<name> -c <container-name> -f # -f is --follow

* Rolling udates and Rollbacks
# Rollout
## rollout > deployment > revision
kubectl rollout status deployment/my-deployment
kubectl rollout history deployment/my-deployment
# Deployment strategy
* Recreate
* Rolling Update - Default deployement strategy
kubectl apply -f deployment-definition.yaml
kubectl set image deployment/my-app-deployment nginx-container=nginx:1.9.1 # careful
kubectl describe deployment/my-deployment
# Rollback
kubectl rollout undo deployment/my-deployment

* Commands
# Dockerfile
FROM ubuntu
ENTRYPOINT ['sleep']
CMD ['5']
Docker            k8s
Entrypoint  ---> command
CMD         ---> args
# yaml
apiVersion: v1
kind: Pod
metadata:
  name: ubumtu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: ['sleepv2']
      args: ['10]

Create ConfigMaps
# imperative
kubectl create configmap <config-name> --from-literal=<key>=<value>
kubectl create configmap <config-name> --from-file=app_config.properties
# declerative
kubectl create -f ..
# yaml - set vars directly in pod
apiVersion: v1
kind: ConfigMap
metadata: 
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod

## yaml - Inject configmap in pod
  ```
  apiVersion: v1
  kind: Pod
  metadata:
    name: simple-webapp-color
  spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    envFrom:
    - configMapRef:
        name: app-config
### yaml - Inject only specific var from configmap in a pod
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: webapp-color
  name: webapp-color
spec:
  containers:
    - image: kodekloud/webapp-color
      imagePullPolicy: Always
      name: webapp-color
      env:
        - name: APP_COLOR
          valueFrom:
            configMapKeyRef:
                # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
              name: webapp-config-map
                # Specify the key associated with the value
              key: APP_COLOR
#### yaml - Inject only specific var from secret in a pod
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
    env:
      - name: MY_SECRET_VAR
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: mysecretkey

* Secrets
kubectl create secret generic <secret-name> --from-literal=<key>=<value> --from-literal=...
kubectl create secret generic app-secret --from-literal=DB_HOST=mysql
kubectl create secret generic app-secret --form-file=app_secret.properties
echo -n "mysql" | base64
echo -n "cm9vdA==" | base64 --decode

# yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mycontainer
    image: nginx
    envFrom:
      - secretRef:
          name: mysecret

* Multi container pods
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: app
  name: app
  namespace: elastic-stack
spec:
  containers:
  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume
  - name: app
    image: kodekloud/event-simulator
    imagePullPolicy: Always
  dnsPolicy: ClusterFirst
  volumes:
  - hostPath:
      path: /var/log/webapp
      type: DirectoryOrCreate
    name: log-volume

There are 3 common patterns, when it comes to designing multi-container PODs. 
The first and what we just saw with the logging service example is known as a side car pattern. 
The others are the adapter and the ambassador pattern.
But these fall under the CKAD curriculum and are not required for the CKA exam.
So we will be discuss these in more detail in the CKAD course.

* Init containers
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']


* Self Healing Applications
Kubernetes supports self-healing applications through ReplicaSets and Replication Controllers. 
The replication controller helps in ensuring that a POD is re-created automatically when the application within the POD crashes. 
It helps in ensuring enough replicas of the application are running at all times.

Kubernetes provides additional support to check the health of applications running within PODs and take necessary actions through Liveness and Readiness Probes. 
However these are not required for the CKA exam and as such they are not covered here. 
These are topics for the Certified Kubernetes Application Developers (CKAD) exam and are covered in the CKAD course.

* Autoscaling
HPA, VPA
- requires metrics server running in the background

Horizontal Pod Autoscaler
kubectl autoscale deployment my-app --cpu-percent=50 --min=1 --max=10
(scales the deployment if the cpu rises above 50% setting replicas from 1 to 10)
- kubectl get hpa
# yaml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  creationTimestamp: null
  name: nginx-deployment
spec:
  maxReplicas: 10
  minReplicas: 1
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  targetCPUUtilizationPercentage: 50
status:
  currentReplicas: 0
  desiredReplicas: 0

In place resize of pod resources
as of v1.32 direct pod update results in pod replacement (delete and recreate)
new feature - 1.27 (alpha), not enabled by default
FEATURE_GATES=InPlacePodVerticalScaling=true (to enable)
only cpu and memory resources can be changed

Vertical Pod Autoscaling
kubectl edit deployment/my-app
change resources
VPA does not come built in with Kubernetes, there is no imperative command
we need to deploy it from kubernetes github
Consists of 3 components:
- admission controller
- recommender
- updater
Operates in 4 modes:
- off, only recommends, does not change anything
- initial, only changes on pod creation, not later
- recreate, evicts pods if usage goes beyond range
- auto, updates existing pods to recommended numbers

kubectl describe vpa my-app

# yaml
apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: hamster-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: hamster
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
          memory: 50Mi
        maxAllowed:
          cpu: 1
          memory: 500Mi
        controlledResources: ["cpu", "memory"]

OS upgrades
kube-control-manager --pod-eviction-timeout=5m0s (default)
kubectl cordon - make unschedulable
kubectl uncordon - make schedulable
kubectl drain - marks unschedulable and evicts pods 

Kubernetes releases
kube-apiserver
controller-manager
kube-scheduler
kubelet
kube-proxy
kubectl (can be different)
these components usually have the same version as the cluster
kube-apiserver is main (v1.10)
controller-manager and kube-scheduler can be (x-1), meaning v1.10 or v1.9
kubelet and kube-proxy can be (x-2), meaning v1.10, v1.9, v1.8
kubectl can be (x+1 > x-1)

References:
https://kubernetes.io/docs/concepts/overview/kubernetes-api/
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md

Cluster Upgrade process
nano /etc/apt/sources.list.d/kubernetes.list -> change to desired version
sudo apt update
sudo apt-cache madison kubeadm -> note the desired version 1.32.0-1.1

sudo apt-mark unhold kubeadm && \
sudo apt-get update && sudo apt-get install -y kubeadm='1.32.0-1.1' && \
sudo apt-mark hold kubeadm

------------------------------------------------------------------------

# upgrade kubelet & kubectl
sudo apt-mark unhold kubelet kubectl && \
sudo apt-get update && sudo apt-get install -y kubelet='1.32.0-1.1' kubectl='1.32.0-1.1' && \
sudo apt-mark hold kubelet kubectl

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# repeat the process for other nodes

# for upgrade run
kubeadm upgrade node

Backup and Restore
Backup candidates:
- Resource configuration
- ETCD cluster
- persistent volumes

- Resource configuration
kubectl get all -A -o yaml > all-deploy-services.yaml
# or velero open source - https://velero.io/

- ETCD cluster
option is to backup --data-dir location, specified when etcd started, like /var/lib/etcd
option is to create a db snapshot, like: 
  ETCDCTL_API=3 etcdctl snapshot save snapshot.db
  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \
      --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \
      snapshot save /tmp/snapshot-pre-boot.db
  # restore
  service kube-apiserver stop
  ETCDCTL_API=3 etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-snapshot
  # when starting the etcd service again, 
    specify --data-dir to point to new location --data-dir /var/lib/etcd-from-snapshot
    # in addition, specify: --endpoints, --cacert, --cert, --key
  systemctl daemon-reload
  service etcd restart
  service kube-apiserver start

Exam tip:
Here's a quick tip. In the exam, you won't know if what you did is correct or not as in the practice tests in this course. 
You must verify your work yourself. 
For example, if the question is to create a pod with a specific image, 
you must run the the kubectl describe pod command to verify the pod is created with the correct name and correct image.

References:
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

https://www.youtube.com/watch?v=qRPNuT080Hk

WORKING WITH ETCDCTL & ETCDUTL
etcdctl is a command line client for etcd.
In all our Kubernetes hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as backup, verify it is running on API version 3.x:

etcdctl version
Example:

controlplane ~ ➜  etcdctl version
etcdctl version: 3.5.16
API version: 3.5
Backing Up ETCD
Using etcdctl (Snapshot-based Backup)
To take a snapshot from a running etcd server, use:

ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /backup/etcd-snapshot.db
Required Options
--endpoints points to the etcd server (default: localhost:2379)

--cacert path to the CA cert

--cert path to the client cert

--key path to the client key

Using etcdutl (File-based Backup)
For offline file-level backup of the data directory:

etcdutl backup \
  --data-dir /var/lib/etcd \
  --backup-dir /backup/etcd-backup
This copies the etcd backend database and WAL files to the target location.

Checking Snapshot Status
You can inspect the metadata of a snapshot file using:

etcdctl snapshot status /backup/etcd-snapshot.db \
  --write-out=table
This shows details like size, revision, hash, total keys, etc. It is helpful to verify snapshot integrity before restore.

Restoring ETCD
Using etcdutl
To restore a snapshot to a new data directory:

etcdutl snapshot restore /backup/etcd-snapshot.db \
  --data-dir /var/lib/etcd-restored
To use a backup made with etcdutl backup, simply copy the backup contents back into /var/lib/etcd and restart etcd.

Notes
etcdctl snapshot save is used for creating .db snapshots from live etcd clusters.

etcdctl snapshot status provides metadata information about the snapshot file.

etcdutl snapshot restore is used to restore a .db snapshot file.

etcdutl backup performs a raw file-level copy of etcd’s data and WAL files without needing etcd to be running.

* Security

ssh-keyget -t rsa

openssl genrsa -out my-key.key 1024
openssl rsa -in my-key.key -pubout > lock.pem

* Certificate Signing Request (CSR)
openssl req -new -key my-key.key -out my-bank.csr \
  -subj "/C=US/ST=CA/O=MyOrg, Inc./CN=my-bank.com"

Public keys:
  *.crt
  *.pem
  
  *.pub

Private keys:
  *.key
  *-key.pem

## Certificate Authority (CA)

- Generate Keys
  ```
  $ openssl genrsa -out ca.key 2048
  ```
- Generate CSR
  ```
  $ openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.csr
  ```
- Sign certificates
  ```
  $ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
  ```

## Generating Client Certificates

#### Admin User Certificates

- Generate Keys
  ```
  $ openssl genrsa -out admin.key 2048
  ```
- Generate CSR
  ```
  $ openssl req -new -key admin.key -subj "/CN=kube-admin" -out admin.csr
  ```
- Sign certificates
  ```
  $ openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
  ```
  
- Certificate with admin privilages
  ```
  $ openssl req -new -key admin.key -subj "/CN=kube-admin/O=system:masters" -out admin.csr
  ```
  
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: akshay
spec:
  signerName: kubernetes.io/kube-apiserver-client
  usages:
    - server auth
  request: 
    LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBN
    QTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJD
    Z0tDQVFFQW16UVlUcFNHc2ZrWFpSVTdKbUV4alBSZjBwRUVya0tKL05CSVhpeDdDVGZBCnd4WmlD
    ZlhBQS9MaUpFVm4xVnRqVWlXUG1ndVVVZ2E2c0Q4MmY1a1h3N2I4Sy90VUVVeS9vL25HRnBhSnQx
    Vi8KZHhwUmZCbEVIcStaNGR0NDJaVXN6dFc1UkRtMnJENGJWSHFQSGUvS1IrbER0NWY0Rkc2dEVX
    aXdwNWJEcitFYQpSTis0bE4rbEFodGxxR2hIb2NNbTZFSHJLWTU3NFBhc0tKUWlncFhMaHhSLzBH
    eHVIMUZQM0dZUU9aa2pvRktOClpwc1NrU1N2Z0xnVmtPWC9ZempGbDNhYnh1NXYxQUQwR3ZoYnQx
    Y1ZIWUlkMjhnTEdXVE82ViszM1h6bDJMcWYKbE9IOEs1Z0pHQjMxTmJQUFo3Ym9uMW5DUVBTT2Nr
    ZC9ramVMSGR4SUJRSURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBSm02NytSaU9X
    SmRUeVl1QmpFTG9EYUY5cGs2Qk9aMFFWVGdJUmJaWktJVU9DaTFIbTNyCkxTdVc4NGc4Y1BDRmJT
    MStGNmpXYnFNUnM1UlE2NGhCZGtEL01yUWRobjNBWW9MdFN5NjV3T21FK29XV3BkNWIKL21uMk4v
    UmowYnZNRFZnclY0anh5UUVGS3lkVS9xTE9nSWYwZGpDVEI3ZFhSNGdlemRLeElwYjVXNEtXR0ha
    WgpUNnJ5VjBZd3c4ejBpbm4rU3dIVkhwWndiTDJrM0QycXl6RERnQjJNT2NHUHkweUtHakV2UDRY
    QVowVFFyMHlCCjdnU1ZsV0xBQmZkYU1rcEJ5V2NEZU9sM09QL29tOVhhV2llWEVJMnpNS1cvVjlL
    Y294WDdxZEl2MUR0bVBCcFgKU290alRld1RXMERheW9OVGJpSlFiaEV6MVEvRTBkbjhrcFU9Ci0t
    LS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=
  expirationSeconds: 600

* rbac
kubectl auth can-i create deployments
kubectl auth can-i delete nodes

kubectl auth can-i delete pods --as dev-user

* Networking

ip link
ip addr
ip addr add 192.168.1.10/24 dev eth0

parsist /etc/network/interfaces 

route
ip route
ip route add 192.168.1.0/24 via 192.168.2.1

# forward requests between interfaces 
cat /proc/sys/net/ipv4/ip_forward 

files:
/etc/hosts - hosts
/etc/resolv.conf - nameservers
/etc/nsswitch.conf - order of ns lookup, files, then dns 

add search property in resolv.conf, specifying the domain. 
when pinging subdomain, it would automatically append the domain in search property

Records:
CNAME - name to name
A - name to IPv4
AAAA - name to IPv6

nslookup 
dig

* Configuring CNI
/opt/cni/bin
/etc/cni/net.d

ls /etc/cni/net.d - show configured plugin 

kube-api server manifest files - to check the configuration for IPs:
/etc/kubernetes/manifests

CoreDNS

/etc/coredns/Corefile